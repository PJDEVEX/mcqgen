{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/chat/openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatopenAI\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\",temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import  SequentialChain\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON response for the chatbot\n",
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\":\"multiple choice question\",\n",
    "        \"options\":{\"a\":\"choice here\",\n",
    "                   \"b\":\"choice here\",\n",
    "                   \"c\":\"choice here\",\n",
    "                   \"d\":\"choice here\"\n",
    "                   },\n",
    "        \"correct\":\"correct_answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\":\"multiple choice question\",\n",
    "        \"options\":{\"a\":\"choice here\",\n",
    "                   \"b\":\"choice here\",\n",
    "                   \"c\":\"choice here\",\n",
    "                   \"d\":\"choice here\"\n",
    "                   },\n",
    "        \"correct\":\"correct_answer\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"mcq\":\"multiple choice question\",\n",
    "        \"options\":{\"a\":\"choice here\",\n",
    "                   \"b\":\"choice here\",\n",
    "                   \"c\":\"choice here\",\n",
    "                   \"d\":\"choice here\"\n",
    "                   },\n",
    "        \"correct\":\"correct_answer\",\n",
    "    },\n",
    "    \"4\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\"\n",
    "        },\n",
    "        \"correct\": \"correct_answer\"\n",
    "    },\n",
    "    \"5\": {\n",
    "        \"mcq\":\"multiple choice question\",\n",
    "        \"options\":{\"a\":\"choice here\",\n",
    "                   \"b\":\"choice here\",\n",
    "                   \"c\":\"choice here\",\n",
    "                   \"d\":\"choice here\"\n",
    "                   },\n",
    "        \"correct\":\"correct_answer\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template 1 for the prompt\n",
    "TEMPLATE =\"\"\"\n",
    "text:{text}\n",
    "You are an expert MCQ maker. Give the above text, it is your job to\\\n",
    "create a quize of {number} multiple choice questions for {subject} in a {tone} tone\\\n",
    "Make sure questions are not repeated and check all question to be confirming to the text as well. \\\n",
    "Make sure to format your resepomse like RESPONSE_JSON below and use it as a guide.  \\\n",
    "Ensure to make {number} MCQs.\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz generation prompt\n",
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    template=TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz generation chain\n",
    "quize_chain = LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template 2 for the prompt\n",
    "TEMPLATE2=\"\"\"\n",
    "Your are an expert English grammerian. Give a multiple choice quiz for {subject} students. \\\n",
    "    you need to evaluate the complexity of the question and give a complete analysis of the quiz. \n",
    "    Only use at max 50 words for the complexity analysis. \\\n",
    "    If the questions are not in par with the student's cognitive and analytical abilities of the student, \\\n",
    "    Update the questions which needs to be changed and change the tone such that it perfectly fits the student's cognitive and analytical abilities. \\\n",
    "    Quiz_MCQs: \n",
    "    {quiz}\n",
    "\n",
    "    check from an expert English writer of the above quiz:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question evaluation prompt\n",
    "quiz_evaluation_prompt = PromptTemplate(\n",
    "    input_variables=[\"subject\", \"quiz\"],\n",
    "    template=TEMPLATE2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiz evaluation chain\n",
    "review_chain = LLMChain(llm=llm, prompt=quiz_evaluation_prompt, output_key=\"review\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential chain for quiz generation and evaluation\n",
    "generate_evaluate_chain = SequentialChain(  \n",
    "    chains=[quize_chain, review_chain],\n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    output_variables=[\"quiz\", \"review\"],\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
